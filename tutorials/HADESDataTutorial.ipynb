{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HADES Data Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "George Marshall, UCL.  Presented at [LEGEND Software Tutorial, Nov. 2021](https://indico.legend-exp.org/event/561/)\n",
    "\n",
    "The most up to date data is located at MPIK in the ggmarsh-test-v03 user production. Hopefully in the next few months a new version will be produced probably at Gran Sasso and copied elsewhere which will the first reference production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LEGEND Production Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full set of tutorials on the production environment can be found here: https://github.com/mmatteo/legend-analysis-tutorials . \n",
    "The github page which provides more detail on all the commands is here: https://github.com/legend-exp/legend-prodenv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have navigated to the production environment at `/lfs/l1/legend/legend-prodenv` , the setup file can be sourced using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`source setup.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives access to all the prodenv commands a full description of which are found above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also found here are the 2 types of production : reference productions and user productions. Currently only user productions are being made but hopefully soon we will have a stable full production pipeline and will then produce the first reference production. The idea is that reference productions will be run using stable versions of pygama whereas the user productions are more experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will focus on the data found at `/lfs/l1/legend/legend-prodenv/prod-usr/ggmarsh-test-v03`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the software for this production cycle we can use:\n",
    "`prodenv-load.sh config.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will put us in the container for this environment with the version of pygama the data was made using. As mentioned earlier for the user productions these may not be stable versions of pygama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Production Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default there are 8 directories in each production cycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataflow contains all the snakmake code which controls the data production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gen is where all the data produced is stored, each detector has a separate directory in which is each tier of production. At the moment data is produced up to and including tier2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genpar contains parameters produced in the data production such as the pole zero constant, calibration constants etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log contains the all the logs for the data production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meta contains all the config files for data productions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src contains all the other software such as the pygama version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "venv is the virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tier1 data contains the following fields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "$ raw.lh5\n",
    "├── stat\n",
    "└── raw\n",
    "     ├── baseline    # FPGA-estimated baseline\n",
    "     ├── channel     # right now, index of the trigger (trace)\n",
    "     ├── energy      # FPGA-estimated energy\n",
    "     ├── ievt        # index of event\n",
    "     ├── numtraces   # number of triggered FADC channels\n",
    "     ├── packet_id   # packet index in file\n",
    "     ├── timestamp   # time since beginning of file\n",
    "     ├── tracelist   # list of triggered FADC channels\n",
    "     ├── waveform    # digitizer data\n",
    "     │   ├── dt      # sampling period (ns) - 16\n",
    "     │   ├── t0\n",
    "     │   └── values  # array holding the waveform samples FADC units\n",
    "     ├── wf_max      # ultra-simple np.max energy estimation\n",
    "     └── wf_std      # ultra-simple np.std noise estimation\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas tier 2 contains a few more:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "$ dps.lh5\n",
    "├── dsp_info\n",
    "└── raw\n",
    "     ├── A_max\n",
    "     ├── QDrift\n",
    "     ├── bl_intercept\n",
    "     ├── bl_mean\n",
    "     ├── bl_slope\n",
    "     ├── bl_std\n",
    "     ├── cuspEftp\n",
    "     ├── cuspEftp_ctc\n",
    "     ├── cuspEmax\n",
    "     ├── cuspEmax_ctc\n",
    "     ├── dt_eff\n",
    "     ├── pz_mean\n",
    "     ├── pz_slope\n",
    "     ├── pz_std\n",
    "     ├── tp_01\n",
    "     ├── tp_0_est\n",
    "     ├── tp_0_trap\n",
    "     ├── tp_10\n",
    "     ├── tp_100\n",
    "     ├── tp_20\n",
    "     ├── tp_50\n",
    "     ├── tp_80\n",
    "     ├── tp_90\n",
    "     ├── tp_95\n",
    "     ├── tp_99\n",
    "     ├── tp_max\n",
    "     ├── tp_min\n",
    "     ├── trapEftp\n",
    "     ├── trapEftp_ctc\n",
    "     ├── trapEmax\n",
    "     ├── trapEmax_ctc\n",
    "     ├── trapTmax\n",
    "     ├── wf_max\n",
    "     ├── wf_min\n",
    "     ├── zacEftp\n",
    "     ├── zacEftp_ctc\n",
    "     ├── zacEmax\n",
    "     └── zacEmax_ctc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full description of how all this data was produced can be found here: https://indico.legend-exp.org/event/698/contributions/3409/attachments/1852/2844/Processing_Chain_lnote_v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bl_mean, bl_slope, bl_std, bl_intercept are parameters related to the baseline namely the mean, RMS, slope and intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pz_mean, pz_slope, pz_std are the same but for the flat top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tp_01, tp_0_est, tp_0_trap, tp_10, tp_100, tp_20, tp_50, tp_80, tp_90, tp_95, tp_99, tp_max, tp_min are all timepoint parameters. tp_0_est and tp_0_trap are two different estimates of the start point of the signal. All others are times to reach the relevant percentage of the energy. Finally tp_max and tp_min are the timepoints of the max and min of the waveform respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuspEftp, cuspEftp_ctc, cuspEmax, cuspEmax_ctc,\n",
    "trapEftp, trapEftp_ctc, trapEmax, trapEmax_ctc, trapTmax, \n",
    "zacEftp, zacEftp_ctc, zacEmax, zacEmax_ctc\n",
    "are all energy estimates. There are three different filter trap, zac and cusp. All filters have been optimised.\n",
    "For each we have 2 different extraction methods the max and fixed time pickoff (ftp). \n",
    "For the moment I would recommend using the max as it performs better for this data.\n",
    "Lastly there are two types ctc and no ctc which is whether the charge trapping correction is included or not.\n",
    "trapTmax is a fixed length trap filter used for timing estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other parameters:\n",
    "A_max  is the max current, \n",
    "QDrift is a parameter related to the uncollected charge. When divided by the energy (trapTmax) we get an effective drift time, dt_eff.\n",
    "Finally wf_max and wf_min are the max and min of the waveform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygama.lh5 as lh5\n",
    "import os,json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find files in this case for a Th lateral scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update these with paths appropriate to NERSC, ideally this directory:\n",
    "# /global/cfs/cdirs/m2676/data/hades/V07646A/tier2/th_HS2_lat_psa\n",
    "# George will edit this soon & remove this message.  --Clint\n",
    "\n",
    "det = 'V07646A'\n",
    "datatype = 'th_HS2_lat_psa'\n",
    "run = '001'\n",
    "datapath = '/lfs/l1/legend/legend-prodenv/prod-usr/ggmarsh-test-v03/gen/'+det+'/tier2/'+datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files      = os.listdir(datapath)\n",
    "files.sort()\n",
    "for i,file in enumerate(files):\n",
    "    files[i] = os.path.join(datapath,file)\n",
    "files = files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sto=lh5.Store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this command to check what is in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sto.ls(files[0], 'raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal = lh5.load_nda(files, ['cuspEmax','cuspEmax_ctc'], 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use dataframes instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lh5.load_dfs(files, ['cuspEmax','cuspEmax_ctc'], 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "counts,bins,bars = plt.hist(uncal['cuspEmax'], bins=10000, histtype='step', label='No Charge Trapping Correction')\n",
    "plt.hist(uncal['cuspEmax_ctc'], bins=bins, histtype='step', label='Charge Trapping Corrected')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Quality Cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygama.genpar_tmp.cuts as cts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_pass, uncal_cut = cts.load_nda_with_cuts(files,'raw',['cuspEmax_ctc'],verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the cuts are generated as a 4 sigma double sided cut on bl_mean, bl_std and pz_std. We can customize by specifying a cut dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cuts_pass, other_cuts_fail  = cts.load_nda_with_cuts(files,'raw',['cuspEmax_ctc'], \n",
    "                                               cut_parameters= {'bl_mean':5,'bl_std':{'left':10,'right':4}, 'pz_std':4},\n",
    "                                                          verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "counts,bins,bars = plt.hist(uncal_pass['cuspEmax_ctc'], bins=1000, histtype='step', label='Passed Cuts')\n",
    "plt.hist(uncal_cut['cuspEmax_ctc'], bins=bins, histtype='step', label='Failed Cuts')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Some detectors have problems with the default cuts as they could not use the DAQ baseline for subtraction. These detectors are V05266A and V04549A, for these just remove bl_mean from the cut dictionary. Also V07647B has a change in baseline value so the first file of run one shouldn't be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Energy Calibration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygama.analysis.calibration as cal\n",
    "import pygama.analysis.peak_fitting as pgp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glines    = [583.191, 727.330, 860.564,1592.53,1620.50,2103.53,2614.50] # gamma lines used for calibration\n",
    "range_keV = [(25,40),(25,40), (25,40),(25,20),(25,40),(25,40),(90,90)] # side bands width\n",
    "funcs = [pgp.radford_peak,pgp.radford_peak,pgp.radford_peak,\n",
    "             pgp.radford_peak,pgp.radford_peak,pgp.radford_peak,pgp.radford_peak]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the calibration we have to specify the peaks to find in keV. The widths around each peak to fit again in keV and finally the function to try and fit to each peak. Here we are just using the radford_peak function which is a gaussian with low energy tail and a step function for the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other argument we have to supply is a guess on the adc to kev conversion. For Th assuming the 99th percentile is around the 2615 peak works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars, cov, results = cal.hpge_E_calibration(\n",
    "    uncal_pass['cuspEmax_ctc'],\n",
    "    glines,\n",
    "    (2620/np.nanpercentile(uncal_pass['cuspEmax_ctc'],99)),\n",
    "    deg=1,\n",
    "    range_keV = range_keV,\n",
    "    funcs = funcs,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecal_pass = pgp.poly(uncal_pass['cuspEmax_ctc'], pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(ecal_pass, bins=10000, histtype='step')\n",
    "plt.xlabel(\"Energy (adc)\",     ha='right', x=1)\n",
    "plt.ylabel(\"Counts / keV\",     ha='right', y=1)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tips: if the routine is struggling to find the peaks then the issue is probably the guess parameter. Here this is specified as `(2600/np.nanpercentile(uncal_pass['cuspEmax_ctc'],99))` where we are guessing that the 99th percentile is around the 2615 peak. This works well for Th but will need different values for other sources. If the peak fitting is struggling try changing the fit widths in the range_keV above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results is a dictionary  containing the location of the found peaks in adc and kev, the location of the fitted peaks in kev and the parameters of the fitted peaks. Finally there is the calibration parameters and the fwhms of the peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively for Th data we can load the energy calibration constants from the ecal files. Note: These won't apply to lower energy sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calpath = '/lfs/l1/legend/legend-prodenv/prod-usr/ggmarsh-test-v03/genpar/dsp_ecal/'+det+'.json'\n",
    "with open(calpath, 'r') as o:\n",
    "    cal_dict = json.load(o)\n",
    "cal_pars = cal_dict['cuspEmax_ctc']['Calibration_pars']\n",
    "eres_pars = [cal_dict['cuspEmax_ctc']['m0'], cal_dict['cuspEmax_ctc']['m1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(pgp.poly(uncal_pass['cuspEmax_ctc'], cal_pars), bins=10000, histtype='step')\n",
    "plt.xlabel(\"Energy (adc)\",     ha='right', x=1)\n",
    "plt.ylabel(\"Counts / keV\",     ha='right', y=1)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pygama there are a number of convenience functions for histogramming and peak fitting. These use by default a least squares fit for binned data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygama.analysis.histograms as pgh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can use the get_hist function to generate the histogram. There are a number of ways to do this you can either specify the number of bins, the edges of the bins or the range and width of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist,bins,var = pgh.get_hist(ecal_pass[(ecal_pass>2554)&(ecal_pass<2664)], range=(2554,2664), dx=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting function is fit_hist. It takes in the function to fit, the data and then a parameter guess to start the fitting. It can also take bounds on these parameters. For energy fitting guessing functions have been implemented so we will use these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use again the radford peak function to fit which is a gaussian with a tail and a step. A number of other peak shapes are implemented in pygama and can be found [here](https://github.com/legend-exp/pygama/blob/master/pygama/analysis/peak_fitting.py). For this fit function, initial guesses have been implemented so we will use these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars_guess = cal.get_hpge_E_peak_par_guess(hist, bins, var, pgp.radford_peak)\n",
    "bounds = cal.get_hpge_E_peak_bounds(hist, bins, var, pgp.radford_peak, pars_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars,covs = pgp.fit_hist(pgp.radford_peak, hist,bins, guess=pars_guess, bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_cs = (bins[1:]+bins[:-1])/2\n",
    "plt.figure()\n",
    "ax1 = plt.subplot(211)\n",
    "plt.xlim([2600,2620])\n",
    "ax1.tick_params('x', labelbottom=False)\n",
    "plt.plot(bin_cs,hist)\n",
    "plt.plot(bin_cs,pgp.radford_peak(bin_cs, *pars))\n",
    "plt.ylabel(\"Counts\")\n",
    "ax2 = plt.subplot(212, sharex=ax1)\n",
    "plt.plot(bin_cs,pgp.radford_peak(bin_cs, *pars)-hist)\n",
    "\n",
    "plt.xlabel(\"Energy (keV)\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using other fit functions there are a number of other helpful functions in pygama to guess parameter values. If fitting a gaussian we can use gauss_mode_width_max to guess the mu, sigma and max. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_pars, g_covs = pgp.gauss_mode_width_max(hist,bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mu guess is {g_pars[0]}, fitted is {pars[0]}')\n",
    "print(f'Sigma guess is {g_pars[1]}, fitted is {pars[1]}')\n",
    "print(f'Max guess is {g_pars[2]}, fitted is {pars[-1]}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
