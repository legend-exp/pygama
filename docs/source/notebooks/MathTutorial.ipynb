{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pygama Math and Fitting\n",
    "\n",
    "The goal of this notebook is to illustrate the conventions of the functions stored in `pygama.math` and how they can be used to fit data. We will also go over how to write new math distributions. \n",
    "\n",
    "## Set up the Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygama.math as math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 4)\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Distributions \n",
    "\n",
    "All statistical distributions are stored in `pygama.math.functions` and can be imported through the module `pygama.math.distributions`. Let's import a plain-old Gaussian and look at the methods it has associated with it! And then, at the end of this section, we can look at the conventions and how to write a new distribution. \n",
    "\n",
    "Let's also fix the shape parameters for the Gaussian.\n",
    "\n",
    "For comparison's sake, let's also import `scipy`'s version of the Gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygama.math.distributions import gaussian\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu = 2.5\n",
    "sigma = 0.7\n",
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "object_methods = [\n",
    "    method_name\n",
    "    for method_name in dir(gaussian)\n",
    "    if callable(getattr(gaussian, method_name))\n",
    "]\n",
    "print(object_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geez, there are a lot of methods. Let's hone in on a couple: \n",
    "### .pdf and .cdf \n",
    "\n",
    "These are just your bread and butter probability density and cumulative density functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_pdf = gaussian.pdf(x, mu, sigma)\n",
    "\n",
    "plt.plot(x, gauss_pdf, label=\"Pygama Gaussian PDF\", c=\"k\")\n",
    "plt.plot(x, norm.pdf(x, mu, sigma), label=\"Scipy Gaussian PDF\", ls=\":\", alpha=1, c=\"r\")\n",
    "plt.title(\"PDF of a Gaussian Distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_cdf = gaussian.cdf(x, mu, sigma)\n",
    "\n",
    "plt.plot(x, gauss_cdf, label=\"Gaussian CDF\", c=\"k\")\n",
    "plt.plot(x, norm.cdf(x, mu, sigma), label=\"Scipy Gaussian CDF\", ls=\":\", alpha=1, c=\"r\")\n",
    "plt.title(\"CDF of a Gaussian Distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that's fine and dandy, what's so special about a class that reproduces `scipy` results? \n",
    "\n",
    "Well, every function in `pygama.math.functions` subclasses `scipy`'s `rv_continuous` class: this allows access to methods that `scipy` automagically computes. Let's look at a few \n",
    "\n",
    "### What `rv_continuous` subclassing gets us: `.rvs`, `.logpdf`, `.sf`... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_log_pdf = gaussian.logpdf(x, mu, sigma)\n",
    "\n",
    "plt.plot(x, gauss_log_pdf, label=\"Gaussian Log PDF\", c=\"k\")\n",
    "plt.plot(\n",
    "    x, norm.logpdf(x, mu, sigma), label=\"Scipy Gaussian Log PDF\", ls=\":\", alpha=1, c=\"r\"\n",
    ")\n",
    "plt.title(\"Log PDF of a Gaussian Distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_sf = gaussian.sf(x, mu, sigma)\n",
    "\n",
    "plt.plot(x, gauss_sf, label=\"Gaussian SF\", c=\"k\")\n",
    "plt.plot(x, norm.sf(x, mu, sigma), label=\"Scipy Gaussian SF\", ls=\":\", alpha=1, c=\"r\")\n",
    "plt.title(\"Survival Fraction of a Gaussian Distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for something really useful, random sampling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = gaussian.rvs(mu, sigma, size=1000, random_state=1)\n",
    "\n",
    "hist, bins = np.histogram(random_sample, bins=100, range=(-10, 10))\n",
    "plt.step(\n",
    "    bins[1:], hist / np.sum(hist) * np.sum(gauss_pdf), label=\"Randomly Sampled Data\"\n",
    ")\n",
    "plt.plot(x, gauss_pdf, label=\"Gaussian PDF\")\n",
    "plt.title(\"Comparison of Randomly Sampled Data with Underlying PDF\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We get access to `scipy`'s `rv_continuous` methods for free\n",
    "\n",
    "If we take a look at the actual code for the definition of `pygama.math.functions.gaussian` we see that there is no direct implementation of methods for `.rvs` or `.logpdf`! We get access to all these methods for free. All we have to do is subclass `rv_continuous` and write definitions for `_pdf` and `_cdf` \n",
    "\n",
    "Now, the drawback: the functions `_pdf` and `_cdf` that `rv_continuous` uses are slow. Let's see this by comparing to the function that actually computes the pdf: \n",
    "### But the `.pdf` and `.cdf` methods are slow! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 4 -n 10000\n",
    "gauss_pdf = gaussian.pdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(x, mu, sigma):\n",
    "    if sigma == 0:\n",
    "        invs = np.inf\n",
    "    else:\n",
    "        invs = 1.0 / sigma\n",
    "    z = (x - mu) * invs\n",
    "    invnorm = invs / np.sqrt(2 * np.pi)\n",
    "    return np.exp(-0.5 * z**2) * invnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 4 -n 10000\n",
    "gauss_pdf = gaussian_pdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast methods: `.get_pdf` and `.get_cdf`\n",
    "\n",
    "Because the overloaded `rv_continuous` implementations of `pdf` and `cdf` are very slow, we have implemented fast versions of these two methods, named `get_pdf` and `get_cdf`. \n",
    "\n",
    "Each of these fast methods call a `numba.njit`-wrapped function --- a just-in-time compiled function --- that runs super fast. \n",
    "\n",
    "We keep the `.pdf` and `.cdf` because they give us access to `scipy` methods we might want to use, but we write these faster methods so that we can fit distributions quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_get_pdf = gaussian.get_pdf(x, mu, sigma)\n",
    "\n",
    "plt.plot(x, gauss_get_pdf, label=\"Pygama Gaussian Get_PDF\", c=\"k\")\n",
    "plt.plot(x, norm.pdf(x, mu, sigma), label=\"Scipy Gaussian PDF\", ls=\":\", alpha=1, c=\"r\")\n",
    "plt.title(\"PDF of a Gaussian Distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's good that it reproduces the scipy results numerically. But how fast does it run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 4 -n 10000\n",
    "gauss_pdf = gaussian.get_pdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, we're even faster than the function we defined above! That's the power of just-in-time compiled code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The remaining methods are helpful for binned and unbinned fitting: `pdf_norm`, `cdf_norm`, `pdf_ext` and `cdf_ext`\n",
    "\n",
    "For `pygama` our default way of fitting distributions is to use the `Iminuit` package. `Iminuit` has several different ways to fit binned and unbinned data by performing either extended or unextended fits. These fitting methods require functions with different properties. The way `pygama` functions interact with `Iminuit` is as follows: \n",
    "\n",
    "1. `pdf_norm` is used for unbinned fits, which require a pdf that is normalized to 1 on the fitting range \n",
    "2. `cdf_norm` is used for binned fits, which require a cdf that derived from a pdf that is normalized to 1 on the fitting range \n",
    "3. `pdf_ext` is used for extended unbinned fits, and the function is required to return a tuple of the support-normalized pdf integrated over the data window, and the scaled support-normalized pdf.\n",
    "4. `cdf_ext` is used for extended binned fits, and just returns the scaled support-derived cdf\n",
    "\n",
    "Let's illustrate some fitting in action, and why some of these weird definitions are required \n",
    "### Import Iminuit and create data to fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import cost, Minuit\n",
    "\n",
    "xr = (-4, 4)  # xrange\n",
    "mu = 0.5\n",
    "sigma = 3\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "xdata = rng.normal(mu, sigma, size=1000)\n",
    "xdata = xdata[(xr[0] < xdata) & (xdata < xr[1])]\n",
    "\n",
    "n, xe = np.histogram(xdata, bins=50, range=xr)\n",
    "cx = 0.5 * (xe[1:] + xe[:-1])\n",
    "dx = np.diff(xe)\n",
    "\n",
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "plt.plot(xdata, np.zeros_like(xdata), \"|\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try an unbinned fit using the correct `pdf_norm` and the incorrect `get_pdf`\n",
    "\n",
    "The method `pdf_norm` requires `x_lo, x_hi`, which are the bounds of the fitting range, to ensure that the pdf is normalized to unity on. So, every  `pdf_norm` method requires that `x_lo` and an `x_hi` are the first two parameters passed to the method call. It's kind of annoying to feed these parameters to Iminuit and fix them each time, but this results in faster fits over saving `x_lo/x_hi` as a state inside our method.\n",
    "\n",
    "We also show a fit with `get_pdf` to show that the fit returns the incorrect results! We keep the `get_pdf` methods in `pygama` because they are useful if we ever need to numerically compute something quickly, or if we do a least squares fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "c = cost.UnbinnedNLL(xdata, gaussian.pdf_norm)\n",
    "\n",
    "m_norm = Minuit(c, x_lo=xr[0], x_hi=xr[1], mu=0.4, sigma=0.2)\n",
    "m_norm.fixed[\"x_lo\", \"x_hi\"] = True\n",
    "m_norm.limits[\"mu\", \"sigma\"] = (0, None)\n",
    "m_norm.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.UnbinnedNLL(xdata, gaussian.get_pdf)\n",
    "\n",
    "m = Minuit(c, mu=0.4, sigma=0.2)\n",
    "m.limits[\"mu\", \"sigma\"] = (0, None)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "xm = np.linspace(*xr)\n",
    "plt.plot(\n",
    "    xm, gaussian.pdf_norm(xm, *m_norm.values) * len(xdata) * dx[0], label=\"pdf_norm fit\"\n",
    ")\n",
    "plt.plot(xm, gaussian.get_pdf(xm, *m.values) * len(xdata) * dx[0], label=\"get_pdf fit\")\n",
    "plt.plot(\n",
    "    xm, gaussian.get_pdf(xm, mu, sigma) * len(xdata) * dx[0], label=\"True Distribution\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `pdf_norm` does a much better job of fitting the actual underlying distribution and reproducing the correct $\\mu,\\sigma$ parameters\n",
    "\n",
    "### try an extended unbinned fit \n",
    "The parameters for the `pdf_ext` are `x_lo, x_hi, area, mu, sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.ExtendedUnbinnedNLL(xdata, gaussian.pdf_ext)\n",
    "\n",
    "m = Minuit(c, x_lo=xr[0], x_hi=xr[1], area=500, mu=0.1, sigma=0.9)\n",
    "m.fixed[\"x_lo\", \"x_hi\"] = True\n",
    "m.limits[\"area\", \"mu\", \"sigma\"] = (0, None)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "xm = np.linspace(*xr)\n",
    "plt.plot(xm, gaussian.pdf_ext(xm, *m.values)[1] * dx[0], label=\"fit\")\n",
    "plt.plot(\n",
    "    xm, gaussian.pdf_ext(xm, xr[0], xr[1], 1000, mu, sigma)[1] * dx[0], label=\"actual\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pdf_ext` function correctly fits the area of the curve! \n",
    "\n",
    "### Binned fits using `cdf_norm` and the incorrect `get_cdf`\n",
    "The arguments are in the order `x_lo, x_hi, mu, sigma` for `cdf_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.BinnedNLL(n, xe, gaussian.cdf_norm)\n",
    "\n",
    "m_norm = Minuit(c, x_lo=xr[0], x_hi=xr[1], mu=0.4, sigma=0.2)\n",
    "m_norm.fixed[\"x_lo\", \"x_hi\"] = True\n",
    "m_norm.limits[\"mu\", \"sigma\"] = (0.1, None)\n",
    "m_norm.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.BinnedNLL(n, xe, gaussian.get_cdf)\n",
    "\n",
    "m = Minuit(c, mu=0.4, sigma=0.2)\n",
    "m.limits[\"mu\", \"sigma\"] = (0.1, None)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "plt.stairs(\n",
    "    np.diff(gaussian.cdf_norm(xe, *m_norm.values)) * len(xdata),\n",
    "    xe,\n",
    "    label=\"cdf_norm fit\",\n",
    ")\n",
    "plt.stairs(\n",
    "    np.diff(gaussian.get_cdf(xe, *m.values)) * len(xdata), xe, label=\"get_cdf fit\"\n",
    ")\n",
    "plt.stairs(\n",
    "    np.diff(gaussian.get_cdf(xe, mu, sigma)) * len(xdata),\n",
    "    xe,\n",
    "    label=\"underlying distribution\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, extended binned fits with `cdf_ext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.ExtendedBinnedNLL(n, xe, gaussian.cdf_ext)\n",
    "\n",
    "m = Minuit(c, area=500, mu=0.1, sigma=0.9)\n",
    "m.limits[\"area\", \"mu\", \"sigma\"] = (0, None)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(gaussian.cdf_ext(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.stairs(np.diff(gaussian.cdf_ext(xe, 1000, mu, sigma)), xe, label=\"underlying\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing our own `pygama.math` distribution: conventions and requirements \n",
    "\n",
    "Let's write a Cauchy distribution. Recall that a Cauchy distribution has support over the real line, and has a pdf like \n",
    "\n",
    "$pdf(x, \\mu,\\sigma) = \\frac{1}{\\pi\\sigma\\left[1+\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right]}$\n",
    "\n",
    "and a cdf:\n",
    "\n",
    "$cdf(x, \\mu,\\sigma) = \\frac{1}{\\pi}\\arctan\\left(\\frac{x-\\mu}{\\sigma}\\right)+\\frac{1}{2}$\n",
    "\n",
    "### What we need to define a distribution class\n",
    "We need four functions that our class methods will call and their arguments, and they all should be numbafied \n",
    "\n",
    "1. A PDF, normalized to the support. Args: x, mu, sigma\n",
    "2. A CDF, derived from the support normalized PDF. Args: x, mu, sigma\n",
    "3. A scaled PDF. Args: x, area, mu, sigma\n",
    "4. A scaled CDF. Args: x, area, mu, sigma\n",
    "\n",
    "The convention is to name these functions `nb_distribution_pdf` and `nb_distribution_scaled_cdf` for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "# here we set some parameters to ensure that numba will compute things as quickly as possible\n",
    "kwd_parallel = {\"parallel\": True, \"fastmath\": True}\n",
    "kwd = {\"parallel\": False, \"fastmath\": True}\n",
    "\n",
    "\n",
    "# define the pdf\n",
    "@nb.njit(**kwd_parallel)\n",
    "def nb_cauchy_pdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        The input data\n",
    "    lamb\n",
    "        The rate\n",
    "    mu\n",
    "        The amount to shift the distribution\n",
    "    sigma\n",
    "        The amount to scale the distribution\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.empty_like(x, dtype=np.float64)\n",
    "    # we want to do a loop because it is faster with parallelization\n",
    "    for i in nb.prange(x.shape[0]):\n",
    "        y[i] = (x[i] - mu) / sigma\n",
    "        y[i] = 1 / (np.pi * sigma * (1 + y[i] ** 2))\n",
    "    return y\n",
    "\n",
    "\n",
    "# define the cdf\n",
    "@nb.njit(**kwd_parallel)\n",
    "def nb_cauchy_cdf(x: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        The input data\n",
    "    lamb\n",
    "        The rate\n",
    "    mu\n",
    "        The amount to shift the distribution\n",
    "    sigma\n",
    "        The amount to scale the distribution\n",
    "    \"\"\"\n",
    "    y = np.empty_like(x, dtype=np.float64)\n",
    "    for i in nb.prange(x.shape[0]):\n",
    "        y[i] = (x[i] - mu) / sigma\n",
    "        y[i] = (1 / np.pi) * np.arctan(y[i]) + 0.5\n",
    "    return y\n",
    "\n",
    "\n",
    "# define the scaled pdf, can't use parallelization here because there is no outer for-loop\n",
    "@nb.njit(**kwd)\n",
    "def nb_cauchy_scaled_pdf(\n",
    "    x: np.ndarray, area: float, mu: float, sigma: float\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        The input data\n",
    "    area\n",
    "        The prefactor to scale the pdf by\n",
    "    lamb\n",
    "        The rate\n",
    "    mu\n",
    "        The amount to shift the distribution\n",
    "    sigma\n",
    "        The amount to scale the distribution\n",
    "    \"\"\"\n",
    "    return area * nb_cauchy_pdf(x, mu, sigma)\n",
    "\n",
    "\n",
    "# define the scaled cdf, can't use parallelization here because there is no outer for-loop\n",
    "@nb.njit(**kwd)\n",
    "def nb_cauchy_scaled_cdf(\n",
    "    x: np.ndarray, area: float, mu: float, sigma: float\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        The input data\n",
    "    area\n",
    "        The prefactor to scale the pdf by\n",
    "    lamb\n",
    "        The rate\n",
    "    mu\n",
    "        The amount to shift the distribution\n",
    "    sigma\n",
    "        The amount to scale the distribution\n",
    "    \"\"\"\n",
    "    return area * nb_cauchy_cdf(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create a class using these four functions \n",
    "\n",
    "Each `pygama` distribution class subclasses `pygama_continuous`, which itself is a subclass of `rv_continuous`. \n",
    "\n",
    "#### An aside on the ordering of parameters: \n",
    "In the following the term \"shape parameter\" refers to a specific variable used to define a distribution that is not an overall scaling (which I call $\\mu$) or shifting (which I call $\\sigma$). An example of shape parameters would be the $\\beta, m$ parameters in a crystal ball function. The location and scale ($\\mu, \\sigma$) parameters have a more specific definition --- akin to what `scipy` does. If a $\\mu, \\sigma$ value is present, then the entire distribution is shifted and scaled; this can be achieved by transforming $y = \\frac{x-\\mu}{\\sigma}$ and computing $pdf(y)/\\sigma$ ( where the factor of $1/\\sigma$ comes from the Jacobian). \n",
    "\n",
    "The ordering of parameters is as follows, using whichever are needed for a function: \n",
    "`x, x_lo, x_hi, area, mu, sigma, shapes, ...`\n",
    "\n",
    "Where `x_lo` and `x_hi` are the lower and upper bounds to evaluate a function on. These are particularly important when the support of a function depends on its fit range; distributions like uniform, linear, and step distributions are defined on a finite support and thus `x_lo` and `x_hi` are parameters that should be passed to *every* function call. \n",
    "\n",
    "\n",
    "#### Now, back to require methods\n",
    "\n",
    "The class name has the format `distribution_name_gen` and it must include the following methods with their arguments:\n",
    "\n",
    "1. `_pdf(self, x, mu, sigma, shapes)`:\n",
    "\n",
    "    This is an overloading of the `scipy` method for the pdf. If shape parameters are present, `nb_distribution_pdf` must also be called with `shape_param[0]` because `scipy` passes an array of shape parameters to the function calls. Finally, `x.flags.writeable = True` must be passed so that `numba` can operate as expected on arrays. \n",
    "    \n",
    "  \n",
    "2. `_cdf(self, x, mu, sigma, shapes)`:\n",
    "\n",
    "    This is an overloading of the `scipy` method for the cdf. The same format applies as for the pdf. \n",
    "       \n",
    "       \n",
    "3. `get_pdf(x, mu, sigma, shapes)`: \n",
    "\n",
    "    This is a direct call to the fast `nb_distribution_pdf`\n",
    "  \n",
    "  \n",
    "4. `get_pdf(x, mu, sigma, shapes)`: \n",
    "\n",
    "    This is a direct call to the fast `nb_distribution_cdf`\n",
    "    \n",
    "    \n",
    "5. `pdf_norm(x, x_lo, x_hi, mu, sigma, shapes)`:\n",
    "\n",
    "    In the case that the support of the distribution is the entire real line, this calls a `pygama_continuous` super method that normalizes the pdf to unity on the range $[x_{lo},x_{hi}]$ by computing $\\frac{pdf(x)}{cdf(x_{hi})- cdf(x_{lo})}$. If the distribution is defined on a limited domain, this function needs to be directly overloaded -- see `pygama.math.functions.step` for an example. \n",
    "    \n",
    "    \n",
    "6. `cdf_norm(x, x_lo, x_hi, mu, sigma, shapes)`:\n",
    "\n",
    "    In the case that the support of the distribution is the entire real line, this calls a `pygama_continuous` super method that derives the cdf from a pdf that is normalized to unity on the range $[x_{lo},x_{hi}]$ by computing $\\frac{cdf(x)}{cdf(x_{hi})- cdf(x_{lo})}$. If the distribution is defined on a limited domain, this function needs to be directly overloaded -- see `pygama.math.functions.step` for an example.\n",
    "    \n",
    "    \n",
    "7. `pdf_ext(x, x_lo, x_hi, area, mu, sigma, shapes)`: \n",
    "\n",
    "    A function that returns both the integral of the support-normalized pdf on the interval $[x_{lo},x_{hi}]$, as well as the support-normalized pdf on that range as well. The fastest way to compute these is to return `np.diff(nb_distribution_scaled_cdf(np.array([x_lo, x_hi]), area, mu, sigma, shapes))` and `nb_distribution_scaled_pdf(x, area, mu, sigma, shapes)`\n",
    "  \n",
    "  \n",
    "8. `cdf_ext(x, area, mu, sigma, shapes)`:\n",
    "\n",
    "    A function that returns just the scaled cdf, `nb_distribution_scaled_cdf(x, area, mu, sigma, shapes)`\n",
    "    \n",
    "    \n",
    "9. `req_args(self)`: \n",
    "\n",
    "    A function that returns a tuple with the strings of the names of the required shape parameters and mu and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygama.math.functions.pygama_continuous import pygama_continuous\n",
    "\n",
    "\n",
    "class cauchy_gen(pygama_continuous):\n",
    "    def _pdf(self, x: np.ndarray) -> np.ndarray:\n",
    "        x.flags.writeable = True\n",
    "        return nb_cauchy_pdf(x, 0, 1)\n",
    "\n",
    "    def _cdf(self, x: np.ndarray) -> np.ndarray:\n",
    "        x.flags.writeable = True\n",
    "        return nb_cauchy_cdf(x, 0, 1)\n",
    "\n",
    "    def get_pdf(self, x: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n",
    "        return nb_cauchy_pdf(x, mu, sigma)\n",
    "\n",
    "    def get_cdf(self, x: np.ndarray, mu: float, sigma: float) -> np.ndarray:\n",
    "        return nb_cauchy_cdf(x, mu, sigma)\n",
    "\n",
    "    # needed so that we can hack iminuit's introspection to function parameter names.\n",
    "    def pdf_norm(\n",
    "        self, x: np.ndarray, x_lo: float, x_hi: float, mu: float, sigma: float\n",
    "    ) -> np.ndarray:\n",
    "        return self._pdf_norm(x, x_lo, x_hi, mu, sigma)\n",
    "\n",
    "    def cdf_norm(\n",
    "        self, x: np.ndarray, x_lo: float, x_hi: float, mu: float, sigma: float\n",
    "    ) -> np.ndarray:\n",
    "        return self._cdf_norm(x, x_lo, x_hi, mu, sigma)\n",
    "\n",
    "    def pdf_ext(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        x_lo: float,\n",
    "        x_hi: float,\n",
    "        area: float,\n",
    "        mu: float,\n",
    "        sigma: float,\n",
    "    ) -> np.ndarray:\n",
    "        return np.diff(\n",
    "            nb_cauchy_scaled_cdf(np.array([x_lo, x_hi]), area, mu, sigma)\n",
    "        ), nb_cauchy_scaled_pdf(x, area, mu, sigma)\n",
    "\n",
    "    def cdf_ext(\n",
    "        self, x: np.ndarray, area: float, mu: float, sigma: float\n",
    "    ) -> np.ndarray:\n",
    "        return nb_cauchy_scaled_cdf(x, area, mu, sigma)\n",
    "\n",
    "    def required_args(self) -> tuple[str, str]:\n",
    "        return \"mu\", \"sigma\"\n",
    "\n",
    "\n",
    "cauchy = cauchy_gen(name=\"cauchy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import cauchy as scipy_cauchy\n",
    "\n",
    "cauchy_pdf = cauchy.pdf(x, mu, sigma)\n",
    "cauchy_get_pdf = cauchy.get_pdf(x, mu, sigma)\n",
    "\n",
    "cauchy_cdf = cauchy.cdf(x, mu, sigma)\n",
    "cauchy_get_cdf = cauchy.get_cdf(x, mu, sigma)\n",
    "\n",
    "plt.plot(x, cauchy_pdf, label=\"Pygama Cauchy PDF\", c=\"k\")\n",
    "plt.plot(x, cauchy_get_pdf, label=\"Pygama Cauchy get_pdf\", c=\"b\", ls=\"--\")\n",
    "plt.plot(\n",
    "    x, scipy_cauchy.pdf(x, mu, sigma), label=\"Scipy Cauchy PDF\", ls=\":\", alpha=1, c=\"r\"\n",
    ")\n",
    "plt.title(\"PDF of a Cauchy Distribution\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(x, cauchy_cdf, label=\"Pygama Cauchy CDF\", c=\"k\")\n",
    "plt.plot(x, cauchy_get_cdf, label=\"Pygama Cauchy get_cdf\", c=\"b\", ls=\"--\")\n",
    "plt.plot(\n",
    "    x, scipy_cauchy.cdf(x, mu, sigma), label=\"Scipy Cauchy CDF\", ls=\":\", alpha=1, c=\"r\"\n",
    ")\n",
    "plt.title(\"CDF of a Cauchy Distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform some fits using the new Cauchy distribution to show the other methods' validity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = (-4, 4)  # xrange\n",
    "mu = 0.5\n",
    "sigma = 0.7\n",
    "\n",
    "xdata = scipy_cauchy.rvs(mu, sigma, size=1000, random_state=42)\n",
    "xdata = xdata[(xr[0] < xdata) & (xdata < xr[1])]\n",
    "\n",
    "n, xe = np.histogram(xdata, bins=50, range=xr)\n",
    "cx = 0.5 * (xe[1:] + xe[:-1])\n",
    "dx = np.diff(xe)\n",
    "\n",
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "plt.plot(xdata, np.zeros_like(xdata), \"|\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unbinned fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.UnbinnedNLL(xdata, cauchy.pdf_norm)\n",
    "\n",
    "m_norm = Minuit(c, x_lo=xr[0], x_hi=xr[1], mu=0.4, sigma=0.2)\n",
    "m_norm.fixed[\"x_lo\", \"x_hi\"] = True\n",
    "m_norm.limits[\"mu\", \"sigma\"] = (0, None)\n",
    "m_norm.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extended unbinned fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.ExtendedUnbinnedNLL(xdata, cauchy.pdf_ext)\n",
    "\n",
    "m = Minuit(c, x_lo=xr[0], x_hi=xr[1], area=500, mu=0.1, sigma=0.9)\n",
    "m.fixed[\"x_lo\", \"x_hi\"] = True\n",
    "m.limits[\"area\", \"mu\", \"sigma\"] = (0.01, None)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binned fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.BinnedNLL(n, xe, cauchy.cdf_norm)\n",
    "\n",
    "m_norm = Minuit(c, x_lo=xr[0], x_hi=xr[1], mu=0.4, sigma=0.2)\n",
    "m_norm.fixed[\"x_lo\", \"x_hi\"] = True\n",
    "m_norm.limits[\"mu\", \"sigma\"] = (0.1, None)\n",
    "m_norm.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended binned fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.ExtendedBinnedNLL(n, xe, cauchy.cdf_ext)\n",
    "\n",
    "m = Minuit(c, area=500, mu=0.1, sigma=0.9)\n",
    "m.limits[\"area\", \"mu\", \"sigma\"] = (0, None)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding distributions with `sum_dists`\n",
    "\n",
    "In the business of fitting data, adding two distributions together is our bread-and-butter. This part of the notebook will show you how to create your own distribution that is an instance of `pygama.math.sum_dists`. There are a couple of different use cases for adding distributions together, and we will look at each in turn, but here is a summary:\n",
    "\n",
    "1. Adding different fractions of distributions together, and fitting out the relative amount of distributions\n",
    "2. Adding distributions with different areas (present in equal fractions) and fitting the areas \n",
    "\n",
    "### Let's first create the synthetic data that we will fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = (-4, 4)  # xrange\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "mu = 0.5\n",
    "sigma = 1.3\n",
    "\n",
    "mu2 = 1\n",
    "sigma2 = 0.6\n",
    "\n",
    "\n",
    "xdata = norm.rvs(mu, sigma, size=1000, random_state=42)\n",
    "ydata = scipy_cauchy.rvs(mu2, sigma2, size=len(xdata), random_state=42)\n",
    "xmix = np.append(xdata, ydata)\n",
    "xmix = xmix[(xr[0] < xmix) & (xmix < xr[1])]\n",
    "\n",
    "n, xe = np.histogram(xmix, bins=50, range=xr)\n",
    "cx = 0.5 * (xe[1:] + xe[:-1])\n",
    "dx = np.diff(xe)\n",
    "\n",
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "plt.plot(xmix, np.zeros_like(xmix), \"|\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little about `sum_dists`\n",
    "The first important thing to note about this class is that all methods in this class are of the form `method(x, *parameter_array)` or `method(x, parameters, ...)`. \n",
    "\n",
    "`sum_dists` works by adding two --- and only two --- distributions together. The first thing a user does in creating a new instance is define the order of elements in a `parameter_array`. Then, `sum_dists` works by grabbing elements at different indices in this `parameter_array` according to rules that the user provides in the instantiation. Here's how to write a generic class: \n",
    "\n",
    "1. Create a `parameter_index_array` that holds the indices of what will eventually come in the `parameter_array`. If the user will eventually pass `parameters = [frac1, mu, sigma]` then we just take `parameter_index_array=[frac1, mu, sigma]=range(3)`\n",
    "\n",
    "2. `sum_dists` takes an alternating pattern of distributions and distribution-specific parameter_index_arrays. Each par array can contain `[mu, sigma, shape]`. These par arrays are placed in a tuple with their distribution like `(dist1, [mu1, sigma1, shape1])`. Finally, a list of these tuples is fed to the constructor as `sum_dists([(dist1, [mu1, sigma1, shape1]), (dist2, [mu2, sigma2])])`\n",
    "\n",
    "3. The `sum_dists` constructor then takes an array corresponding the index locations of where either fraction or area values will be passed in the ultimate `parameter_index_array`.\n",
    "\n",
    "4. We pass one of the 4 flag options, to be described below. \n",
    "\n",
    "5. We can optionally pass a list of parameter names, but these can also be calculated automatically if left as None\n",
    "\n",
    "Let's trace through how the code works for a simple example. Suppose we want to create the following distribution \n",
    "\n",
    "$new\\_pdf(x, \\mu, \\sigma, \\tau, frac_1) = frac_1\\cdot dist_1(x, \\mu, \\sigma, \\tau) + (1-frac1)\\cdot dist_2(x, \\mu, \\sigma)$\n",
    "\n",
    "The first thing we would do is create our `parameter_index_array` \n",
    "\n",
    "`[mu, sigma, tau, frac_1] = range(4)`\n",
    "\n",
    "Then, we would create our alternating pattern of distributions and parameter index arrays:\n",
    "\n",
    "`args = [(dist1, [mu, sigma, tau]), (dist2, [mu, sigma])]`\n",
    "\n",
    "Finally, we would intitalize (with the `fracs` flag in this case, more on that later) \n",
    "\n",
    "`sum_dists(args, area_frac_idxs = [frac_1], frac_flag = \"fracs\", parameter_names=[\"mu\", \"sigma\", \"tau\", \"frac_1\"])`\n",
    "\n",
    "\n",
    "### So... What is `sum_dists` actually doing? \n",
    "Under the hood, `sum_dists` is applying a set of rules so that the following *is always* computed, regardless of the flag sent to the constructor. \n",
    "\n",
    "`area1*frac1*dist1(x, mu, sigma, shape) + area2*frac2*dist_2(x, mu2, sigma2, shape2)`\n",
    "\n",
    "It computes this by first grabbing the relevant areas or fraction values from their position in the `parameter_index_array`. Then, at the time of method call, `sum_dists` grabs the values from `parameter_index_array` that correspond to each distribution via slicing the `parameter_index_array` with the individual par arrays passed in the instantiation. In our example above, `sum_dists` knows to grab the values at indices 0, 1, 2 for the first distribution because it is instantiated with the tuple `(dist1, [mu, sigma, tau])`.\n",
    "\n",
    "There's also some work done to determine which of `area` and `frac` are present. That's the purpose of the `flag`. Let's take some time and learn a little more about what each flag does. \n",
    " \n",
    "\n",
    "\n",
    "### The flag in the `sum_dists` constructor\n",
    "Let's say we are interested in knowing the amount of counts present in a signal and a background in our total spectrum, i.e. we want to create and fit a function that looks like\n",
    "\n",
    "$pdf= A\\cdot gauss\\_pdf + B\\cdot cauchy\\_pdf$\n",
    "\n",
    "Because we are interested in fitting the areas, we send the `areas` keyword to `flag`. This causes `sum_dists` to look for an `area_frac_idxs` array of length 2 in the instantiation: this array contains the indices that the area values will be located at in the `parameter_index_array`. The `areas` keyword causes the fractions to be set to `1`. \n",
    "\n",
    "In our example above for constructing $new\\_pdf(x, \\mu, \\sigma, \\tau, frac_1) = frac_1\\cdot dist_1(x, \\mu, \\sigma, \\tau) + (1-frac1)\\cdot dist_2(x, \\mu, \\sigma)$, the instantiation takes the `fracs` keyword. This causes `sum_dists` to look for an `area_frac_idxs` array of length 1 in the instantiation: this array contains the index that the fraction values will be located at in the `parameter_index_array`. `sum_dists` works by multiplying `frac` times the first distribution, and `1-frac` times the second distribution.\n",
    "\n",
    "## An aside on conventions\n",
    "The ultimate `parameter_index_array` that the user passes to the methods *should* look like `x_lo, x_hi, area/frac_1, mu, sigma, shapes1, area/frac2, mu2, sigma2, shapes2` based on which are present. For distributions with ill-defined support, such as a step or linear function, the user must always pass `x_lo, x_hi` for every method call such as `gauss_on_step.get_pdf`. If a sum_dists instance is created from distributions with well-defined support, then `sum_dist` *requires* that for `pdf_norm, cdf_norm, pdf_ext` that `x_lo, x_hi` are added at the front of the parameter index array; for ill-defined distributions it knows to look for `x_lo, x_hi` at the user specified location. This means that for `pdf_norm, cdf_norm, pdf_ext` the fit/normalization range is always set equal to the support range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygama.math.functions.sum_dists import sum_dists\n",
    "\n",
    "# we first create an array contains the indices of the parameter array\n",
    "# that will eventually be input to function calls\n",
    "(area1, mu, sigma, area2, mu2, sigma2) = range(6)\n",
    "\n",
    "# we now create an array containing the tuples of the distributions and their shape parameters\n",
    "args = [(gaussian, [mu, sigma]), (cauchy, [mu2, sigma2])]\n",
    "# we initialize with the flag = \"areas\" to let the constructor know we are sending area parameter idxs only\n",
    "cauchy_on_gauss = sum_dists(\n",
    "    args,\n",
    "    area_frac_idxs=[area1, area2],\n",
    "    flag=\"areas\",\n",
    "    parameter_names=[\"area1\", \"mu\", \"sigma\", \"area2\", \"mu2\", \"sigma2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That was super quick, let's see how it does with fitting data\n",
    "\n",
    "Because we are interested in fitting out areas, we need to use the extended forms of our fits \n",
    "And for `pdf_ext` we need to pass `x_lo, x_hi` as the first two values, and fix them before fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.ExtendedUnbinnedNLL(xmix, cauchy_on_gauss.pdf_ext)\n",
    "\n",
    "m = Minuit(c, xr[0], xr[1], 1000, 0.5, 1.3, 1000, 1, 2)\n",
    "m.fixed[0, 1] = True\n",
    "m.limits[2, 5] = (0, None)\n",
    "m.limits[3, 5, 6, 7] = (0, 2)\n",
    "print(m.migrad())\n",
    "\n",
    "\n",
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "xm = np.linspace(*xr)\n",
    "plt.plot(xm, cauchy_on_gauss.pdf_ext(xm, *m.values)[1] * dx[0], label=\"fit\")\n",
    "plt.plot(\n",
    "    xm,\n",
    "    cauchy_on_gauss.pdf_ext(xm, xr[0], xr[1], 1000, 0.5, 1.3, 1000, 1, 0.6)[1] * dx[0],\n",
    "    label=\"actual\",\n",
    ")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's pretty good! Let's see how the extended unbinned fit fairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.ExtendedBinnedNLL(n, xe, cauchy_on_gauss.cdf_ext)\n",
    "m = Minuit(c, 100, 0.1, 0.3, 100, 1, 2)\n",
    "m.limits[0, 3] = (0, None)\n",
    "m.limits[1, 2, 4, 5] = (0, 2)\n",
    "print(m.migrad())\n",
    "\n",
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(cauchy_on_gauss.cdf_ext(xe, *np.array(m.values))), xe, label=\"fit\")\n",
    "plt.stairs(\n",
    "    np.diff(cauchy_on_gauss.cdf_ext(xe, *np.array([1000, 0.5, 1.3, 1000, 1, 0.6]))),\n",
    "    xe,\n",
    "    label=\"actual\",\n",
    ")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `fracs` flag in the `sum_dists` constructor\n",
    "To get a feel for how `sum_dists` works, let's create a function that creates the following distribution:\n",
    "\n",
    "$pdf = f_1\\cdot gauss\\_{pdf}+(1-f_1)\\cdot cauchy\\_{pdf}$\n",
    "\n",
    "The `fracs` keyword allows `sum_dists` to look for the parameter index corresponding to the value of one fraction, and then automatically calculates `f*dist1+(1-f)*dist2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygama.math.functions.sum_dists import sum_dists\n",
    "\n",
    "\n",
    "# we first create an array contains the indices of the parameter array\n",
    "# that will eventually be input to function calls\n",
    "(frac1, mu, sigma, mu2, sigma2) = range(5)\n",
    "\n",
    "# we now create an array containing the distributions and their shape parameters\n",
    "args = [(gaussian, [mu, sigma]), (cauchy, [mu2, sigma2])]\n",
    "# we initialize with the flag = \"areas\" to let the constructor know we are sending area parameters idxs only\n",
    "cauchy_on_gauss = sum_dists(\n",
    "    args,\n",
    "    area_frac_idxs=[frac1],\n",
    "    flag=\"fracs\",\n",
    "    parameter_names=[\"frac1\", \"mu\", \"sigma\", \"mu2\", \"sigma2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, that was painless; let's see how well it does fitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.UnbinnedNLL(xmix, cauchy_on_gauss.pdf_norm)\n",
    "\n",
    "m = Minuit(c, xr[0], xr[1], 0.5, 0.1, 0.2, 1, 0.1)\n",
    "m.fixed[0, 1] = True\n",
    "m.limits[2] = (0, 1)\n",
    "m.limits[3, 4, 5, 6] = (0.5, 2)\n",
    "print(m.migrad())\n",
    "\n",
    "plt.errorbar(cx, n, n**0.5, fmt=\"ok\")\n",
    "xm = np.linspace(*xr)\n",
    "plt.plot(\n",
    "    xm,\n",
    "    cauchy_on_gauss.pdf_norm(xm, *np.array(m.values)) * len(xmix) * dx[0],\n",
    "    label=\"fit\",\n",
    ")\n",
    "plt.plot(\n",
    "    xm,\n",
    "    cauchy_on_gauss.get_pdf(xm, *np.array([0.5, 0.5, 1.3, 1, 0.6])) * len(xmix) * dx[0],\n",
    "    label=\"actual\",\n",
    ")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By using `sum_dists` on an instance of `sum_dists`, you can even sum three distributions together\n",
    "See the `hpge_peak` function to see an example of this. \n",
    "\n",
    "There is also one other `flag` keyword that `sum_dists` can take: `one_area`. This special keyword is used if we have an odd number of distributions that we want to add together and fit their areas. The first two distributions can be an instance of `sum_dists` with the `areas` flag; however, to add a third distribution to this, we need a way to pass only one area idx to the instantiation of `sum_dists`. See the `triple_gauss_on_double_step` function for an example of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "Hopefully you have learned how to use the distributions and tools packaged in `pygama` for your own scientific purposes. If there's a distribution you see is missing, feel free to contribute it using the conventions described above! "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
